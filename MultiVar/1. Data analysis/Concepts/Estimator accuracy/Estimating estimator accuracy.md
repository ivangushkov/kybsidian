We train theta_hat. Can we estimate how accurate our estimate is?

This goes back to [[Bias variance tradeoff]], and neighborhood variations in the domain of the datset, and how it maps to the estimator space.

We may wish to do this to provide confidence levels about the model parameters to the "users"

It will also aid reflection on the structure of the data, i.e. outliers and skewness of the estimated probability distributions


We may do this by f.e.x
[[Jackknifing]]
[[Bootstrapping]]
[[Cross validation]]

although the performance is usually negligible